<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Data Lakes: Some thoughts on Hadoop, Hive, HBase, and Spark - Vitthal Mirji</title><link rel=icon type=image/png href=favicon.ico><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Data Lakes: Some thoughts on Hadoop, Hive, HBase, and Spark"><meta itemprop=description content="This article will talk about how organizations can make use of the wonderful thing that is commonly referred to as “Data Lake” - what constitutes a Data Lake, how probably should (and shouldn’t) use it to gather insights and why evaluating technologies is just as important as understanding your data..."><meta itemprop=datePublished content="2017-11-04T00:00:00+00:00"><meta itemprop=dateModified content="2017-11-04T00:00:00+00:00"><meta itemprop=wordCount content="4969"><meta itemprop=keywords content="Big Data,Hbase,Hive,Spark,Scala,Phoenix,Data Lake,Hadoop"><meta property="og:url" content="https://vitthalmirji.com/2017/11/data-lakes-some-thoughts-on-hadoop-hive-hbase-and-spark/"><meta property="og:site_name" content="Vitthal Mirji"><meta property="og:title" content="Data Lakes: Some thoughts on Hadoop, Hive, HBase, and Spark"><meta property="og:description" content="This article will talk about how organizations can make use of the wonderful thing that is commonly referred to as “Data Lake” - what constitutes a Data Lake, how probably should (and shouldn’t) use it to gather insights and why evaluating technologies is just as important as understanding your data..."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-11-04T00:00:00+00:00"><meta property="article:modified_time" content="2017-11-04T00:00:00+00:00"><meta property="article:tag" content="Big Data"><meta property="article:tag" content="Hbase"><meta property="article:tag" content="Hive"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Scala"><meta property="article:tag" content="Phoenix"><meta name=twitter:card content="summary"><meta name=twitter:title content="Data Lakes: Some thoughts on Hadoop, Hive, HBase, and Spark"><meta name=twitter:description content="This article will talk about how organizations can make use of the wonderful thing that is commonly referred to as “Data Lake” - what constitutes a Data Lake, how probably should (and shouldn’t) use it to gather insights and why evaluating technologies is just as important as understanding your data..."><link rel=stylesheet type=text/css media=screen href=https://vitthalmirji.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://vitthalmirji.com/css/main.css><link rel=stylesheet type=text/css href=https://vitthalmirji.com/[css/override.css]><link id=dark-scheme rel=stylesheet type=text/css href=https://vitthalmirji.com/css/dark.css><script src=https://vitthalmirji.com/js/feather.min.js></script><script src=https://vitthalmirji.com/js/main.js></script></head><body><div class="container-wide wrapper"><link rel=icon href=../../../favicon.ico sizes=any><link rel=icon type=image/png href=../../../favicon.png><link rel=apple-touch-icon href=../../../apple-touch-icon.png><link rel=manifest href=../../../site.webmanifest><meta name=theme-color content="#ffffff"><link rel=stylesheet href=../../../syntax.css><div class=header><h1 class=site-title><a href=https://vitthalmirji.com/>Vitthal Mirji</a></h1><div class=site-description><p>Software Engineering, Data Engineering, GNU/Linux, Data, ML, and other things</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/vim89/ title=Github><i data-feather=github></i></a></li><li><a href=https://twitter.com/whoami_vim/ title=Twitter><i data-feather=twitter></i></a></li><li><a href=https://www.linkedin.com/in/vitthal10/ title=LinkedIn><i data-feather=linkedin></i></a></li><li><a href=../../../index.xml title=RSS><i data-feather=rss></i></a></li><span class=scheme-toggle><a href=# id=scheme-toggle></a></ul></nav></div><nav class=nav><ul class=flat><li><a href=../../../>Home</a></li><li><a href=../../../about>About</a></li><li><a href=../../../tags>Tags & Stats</a></li></ul></nav></div><div class=article-nav id=article-nav-id><div class=post><div class=post-header><div class=meta><div class=date><span class=day>04</span>
<span class=rest>Nov 2017</span></div></div><div class=matter><h1 class=title>Data Lakes: Some thoughts on Hadoop, Hive, HBase, and Spark</h1></div></div><aside class=toc id=static-toc><header><h3>Contents</h3></header><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#utilizing-your-data-lake>Utilizing your Data Lake</a></li><li><a href=#a-short-prefix-for-apache-hive>A short prefix for Apache Hive</a></li><li><a href=#hive>Hive</a></li><li><a href=#hive-on-9656995-records>Hive on 9,656,995 records</a></li><li><a href=#hbase-and-other-non-relational-databases>HBase and other non-relational databases</a></li><li><a href=#sql-on-nosql>SQL on noSQL</a></li><li><a href=#apache-spark>Apache Spark</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></aside><h2 id=introduction>Introduction</h2><p><em>This article will talk about how organizations can make use of the wonderful thing that is commonly referred to as
“Data Lake” - what constitutes a Data Lake, how probably should (and shouldn’t) use it to gather insights and why
evaluating technologies is just as important as understanding your data.</em></p><p>When organizations talk about the need to utilize data as part of their IT and business strategy, they usually have
certain goals in mind. A common question usually boils down to “How can we make use of the data that we have available
within our organization?”</p><p>While it might seem like a simple enough question to solve, the devil’s in the detail.</p><ul><li>Where do we store the data? How many systems are currently in use? How are they connected to our business processes?</li><li>Who knows about the data? Who understands that data? Do we have metadata available?</li><li>Which technologies are in use? Who are the people working with these technologies?</li><li>Which systems interact with my Data Lake? Where’s my MDM, CRM, ERP - and where’s Waldo? How do they work, what kind of
data do they store and process?</li><li>What are my ETL requirements? Who can design our data models? Which systems are responsible for these tasks?</li><li>What regulations are impacting our data?</li></ul><p>Given you are able to answer these questions, the next logical step might be to start a consolidation effort. What used
to be the realm of traditional Data Warehouse solutions is now commonly replaced by what we call a “Data Lake” - meaning
an organized, central storage for all your organization’s useful data in a scaleable, powerful repository, often
realized by utilizing Hadoop or Cloud-based Big Data toolsets like Google BigQuery.</p><p>Now, let’s just assume you tackled all of these issues – your data is on your cluster, half a billion scheduled Sqoop
jobs merrily import data, sanity checks, ETL, CDC, run like a charm, all regulations are taken care of, Kerberos is
terrorizing IT departments across the globe and everything is living in one place. But… what now, exactly?</p><h2 id=utilizing-your-data-lake>Utilizing your Data Lake</h2><p>When it comes to actually using your Lake, things become really interesting. Usually, you would be utilizing one of the
“standard” tools available with, say, Cloudera’s or Hortonwork’s respective distributions that enable you to access your
data.</p><p>Just to pick a few -</p><ul><li>Hive</li><li>HBase</li><li>Phoenix</li><li>Cassanadra</li><li>Accumulo</li><li>Impala</li><li>Kudu</li><li>Spark</li><li>Pig</li></ul><p>Every single one of those tools has different use cases, different requirements for both technical and human resources
and usually fits a single, usually fairly specific type of data access. So – how can we decide what tool fits best?</p><p>In the next sections, we will cherry-pick technologies and talk about Hive, HBase (plus a bit of Phoenix), and Spark and
apply common use cases on a semi-large (9.6M records) public data set. At the end, we will summarize our findings and
performance tests.</p><h2 id=a-short-prefix-for-apache-hive>A short prefix for Apache Hive</h2><p>When it comes to Hadoop, the transition is hard. One might argue that one of the, if not the, hardest thing about a
transition from traditional DWH to Hadoop is not the technology, but rather the way people use it. That being said, the
first thing that comes to mind when you transition from relational systems to Hadoop might just be Apache Hive – a
mostly ANSI-SQL compliant tool that allows you to fire SQL queries on your HDFS data.</p><p>First off: Hive is a fantastic piece of software that comes with a lot of flexibility. It runs on your laptop (well, it
does on mine&mldr;), on a legacy cluster powered by good old M/R2 just like it does on a modern, Spark or Tez fueled
powerhouse. It makes it easy to work with any data you might have, from legacy CSVs or AS400 Sqoop imports to complex
Avro-magic.</p><p>So, why not just use Hive like you would, say, a Microsoft SQL Server?</p><p>Because <strong>Hive. Is. Not. A. RDBMS.</strong> I cannot stress this enough. Hive uses Schema-On-Read mechanics, does not have the
concept of keys (and that is, well, key!) and is not ACID compatible out of the box. You are also responsible for
managing the underlying system. In the next section, we will dive into the details of what exactly that means.</p><h2 id=hive>Hive</h2><p>We will use the following data
set: <a href=http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml>http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml</a></p><p>Data about the famous NYC cabs!</p><p>For 3 months, 2017-04 to 2017-06 for all Yellow Cabs, it boils down to a total of 29,805,311 records in 2.5GiB. If we
start with June, we still get 813 MiB and 9,656,995 records.</p><p>As we will re-use the same data set for HBase later, you will need to make sure your HBase Key is unique – if you just
use the first value in the CSV, VendorID, you will get a total of 3 records – as there are 3 distinct values in that CSV
for this column. So we need to add an ID using awk. I suggest using head -1000 and piping that to a separate file for
testing if you want to follow along.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2017-06.csv
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>awk -F<span class=s1>&#39;,&#39;</span> -v <span class=nv>OFS</span><span class=o>=</span><span class=s1>&#39;,&#39;</span> <span class=s1>&#39;
</span></span></span><span class=line><span class=cl><span class=s1>NR == 1 {print &#34;ID&#34;, $0; next}
</span></span></span><span class=line><span class=cl><span class=s1>{print (NR-1), $0}
</span></span></span><span class=line><span class=cl><span class=s1>&#39;</span> yellow_tripdata_2017-06.csv &gt; yellow_tripdata_2017-06_id.csv</span></span></code></pre></td></tr></table></div></div><p>Now, we need to create a DDL and import the data:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>DROP</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=k>IF</span><span class=w> </span><span class=k>EXISTS</span><span class=w> </span><span class=n>nyc</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>nyc</span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>id</span><span class=w> </span><span class=nb>BIGINT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>VendorID</span><span class=w> </span><span class=nb>BIGINT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tpep_pickup_datetime</span><span class=w> </span><span class=k>TIMESTAMP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tpep_dropoff_datetime</span><span class=w> </span><span class=k>TIMESTAMP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>passenger_count</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>trip_distance</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>RatecodeID</span><span class=w> </span><span class=n>STRING</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>store_and_fwd_flag</span><span class=w> </span><span class=n>STRING</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>PULocationID</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>DOLocationID</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>payment_type</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>fare_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>extra</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>mta_tax</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tip_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tolls_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>improvement_surcharge</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>total_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>ROW</span><span class=w> </span><span class=n>FORMAT</span><span class=w> </span><span class=n>DELIMITED</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>FIELDS</span><span class=w> </span><span class=n>TERMINATED</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=s1>&#39;,&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>STORED</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>TEXTFILE</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>LOAD</span><span class=w> </span><span class=k>DATA</span><span class=w> </span><span class=n>INPATH</span><span class=w> </span><span class=s1>&#39;hdfs:///user/cloudera/yellow_tripdata_2017-06_id.csv&#39;</span><span class=w> </span><span class=n>OVERWRITE</span><span class=w> </span><span class=k>INTO</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>nyc</span><span class=p>;</span></span></span></code></pre></td></tr></table></div></div><p>Oops. Not good. Wait, let’s just enable transactions! However, you will have issues doing that using Cloudera’s
distribution. Take this exempt from the documentation:</p><blockquote><p>_“The CDH distribution of Hive does not support
transactions (<a href=https://issues.apache.org/jira/browse/HIVE-5317 target=_blank rel="nofollow noopener noreferrer">HIVE-5317</a>
). Currently, transaction support in Hive is
an experimental feature that only works with the ORC file format. Cloudera recommends using the Parquet file format,
which works across many tools. Merge updates in Hive tables using existing functionality, including statements such as
INSERT, INSERT OVERWRITE, and CREATE TABLE AS SELECT.”</p></blockquote><p>(<a href=https://www.cloudera.com/documentation/enterprise/5-12-x/topics/hive_ingesting_and_querying_data.html target=_blank rel="nofollow noopener noreferrer">https://www.cloudera.com/documentation/enterprise/5-12-x/topics/hive_ingesting_and_querying_data.html</a>
)_</p><p>In the interest of transparency, here is how it looks on a manual Hive setup directly on Fedora (On a related note:
Hortonwork’s HDP does support it out of the box):</p><p><img src=images/Selection_048.png alt>
Hive ACID Enabled</p><p>If you take a look at the official Hive documentation, the limitations become even more apparent:</p><ul><li>Only<a href=https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC target=_blank rel="nofollow noopener noreferrer">ORC file format</a>
is supported in this first
release. The feature has been built such that transactions can be used by any storage format that can determine how
updates or deletes apply to base records (basically, that has an explicit or implicit row id), but so far the
integration work has only been done for ORC.</li><li>By default transactions are configured to be off. See
the[Configuration](<a href=https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions%22 target=_blank rel="nofollow noopener noreferrer">https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions"</a>
\l "
HiveTransactions-Configuration)section below for a discussion of which values need to be set to configure it.</li><li>Tables must be<a href=https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables target=_blank rel="nofollow noopener noreferrer">bucketed</a>
to make
use of these features. Tables in the same system not using transactions and ACID do not need to be bucketed. External
tables cannot be made ACID tables since the changes on external tables are beyond the control of the
compactor (<a href=https://issues.apache.org/jira/browse/HIVE-13175 target=_blank rel="nofollow noopener noreferrer">HIVE-13175</a>
).</li><li>Reading/writing to an ACID table from a non-ACID session is not allowed. In other words, the Hive transaction manager
must be set to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager in order to work with ACID tables</li><li>([https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-ConfigurationValuestoSetforINSERT,UPDATE,DELETE](<a href=https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions%22 target=_blank rel="nofollow noopener noreferrer">https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions"</a>
\l &ldquo;HiveTransactions-ConfigurationValuestoSetforINSERT,UPDATE,DELETE))</li></ul><p>While reasonable, it does strengthen my argument – Hive is not an RDMBS, even though you might be importing relational
data. ACID compatibility is possible, but limited to some fairly specific use cases.</p><p>As long as you are using bucketed (often via a Primary Key from source), ORC-based, managed tables on a distribution
that supports this, you are doing great. If you start with more uncommon Hadoop use cases, you might – will – run into
issues.</p><p>So, how do organizations get around this? Usually, you can
play <a href=http://mathworld.wolfram.com/TowerofHanoi.html>The Tower Of Hanoi</a>
– meaning, using temporary tables and
sub-selects to work with INSERT OVERWRITE statements, use a different storage and execution strategy that enables random
writes and updates (more below) – or you can make this a core target and focus on matching the requirements outlined
above.</p><p>When going for the first approach, there might be some…<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Cannot insert into target table because column number/types are different</span></span></code></pre></td></tr></table></div></div>&mldr;errors and excluding columns still involves regex, but after all, a lot is possible if you put your mind to it.</p><p>In any case, sooner or later you will find yourself in need of working with your now accessible data. If you call this
process “mapping”, “transformation” or just throw your entire ETL workload on Hadoop, eventually you will want to gain
some form of insights from your Lake – and usually, this involves Joins, Views and other SQL operations.</p><h2 id=hive-on-9656995-records>Hive on 9,656,995 records</h2><p>Let’s get back to our data. Always keep in mind that this data is currently stored as plain text.</p><p>Let’s try to get all trips with a tip over $100 – it’s NYC, after all:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>with tmp as<span class=o>(</span>
</span></span><span class=line><span class=cl>SELECT * FROM Nyc
</span></span><span class=line><span class=cl>WHERE tip_amount &gt;<span class=o>=</span> <span class=m>100</span>
</span></span><span class=line><span class=cl><span class=o>)</span>
</span></span><span class=line><span class=cl>SELECT COUNT<span class=o>(</span>*<span class=o>)</span>, max<span class=o>(</span>tip_amount<span class=o>)</span>, total_amount as max_tip from tmp group by total_amount<span class=p>;</span></span></span></code></pre></td></tr></table></div></div><p><img src=images/word-image-1.png alt>
Hive on 9M records</p><p>Time taken: 66.138 seconds, Fetched: 185 row(s)</p><p>We got a result in about a minute on a single node <em>(cluster configuration can be found down below)</em>!</p><p>As you can see, I use a less-than-shiny, white-on-black CLI. So, let’s assume we have business users and analysts which
are used to complex, powerful tools such as MS SQL Management Studio.</p><p>We can provide access to business users using Ambari (Hortonworks), Hue, Zeppelin or other web-based front ends.</p><p><a href=images/Selection_050-1024x566.png></a>Hue</p><p>However - storing queries, simply exporting results, auto-complete, stored procedures, special characters and (in most
cases) visualization are not features that are commonly found on these tools.</p><p>You will need an external tool, usually connected via JDBC, for that. Examples include Dbeaver, Squirrel or
Dbvisualizer. Keep in mind that these tools need to talk to Hive (standard port 10000) and are usually Kerberos
enabled – and generally speaking, Kerberos really, really hates nice things. It also hates bad things, in many cases,
that would be Windows.</p><p>To get a bit more serious – <strong>transitioning your seasoned analysts to Hive is not an easy task.</strong> The technology is
fundamentally different, the available tool sets do not offer the comfort many of us got used to over the years and
seemingly simple functionality appears to be missing. It is really challenging to explain that Hive does, in fact, does
not have the concept of “NULLable fields” or even Primary Keys, not to mention the highlighted ACID problematic or
missing features from other SQL dialetcs.</p><p>However: Hive offers different benefits. It is heavily optimized for huge datasets and will not blow up for very large
tables (the 9M records from our example might be a good start for a VM, but is not a representative volume!), can be
easily extended by simply plugging more machines in your cluster, is fully Open Source, can directly work on various
other data sources that are not managed by Hive (External Tables, see below), is integrated in basically any Big Data
related technology (Sqoop even creates your tables and DDLs for you when importing data from an RDBMS), can be your
entry point for Machine Learning on structured data, and serves as key component in any data lake.</p><p><strong>So: You should use Hive</strong>. But do not expect it to work like MariaDB or Oracle. Do not expect setting up a Data Lake
involving a couple of Hadoop Experts that know Hive in-and-out and to receive good feedback from business. As always,
communication is key.</p><p>Use Hive to access your data, to transform your data (though not necessarily directly via the CLI), to provide external
access via JDBC to end users (using the right tool – OpenSource or proprietary), and as major workhorse for your Data
Lake for anything that even resembles relational data.</p><p>But remember – it is not an RDMBS. It was never meant to be that. It is not going to replace anything – it is a piece of
technology that requires some careful planning and coordination between both IT and business. Maybe you even want to
move your results to a different database, visualization tool or warehouse that works better with your Analyst’s toolset
of choice – it all depends on what you are trying to do. But ultimately, you will be rewarded with a system which
bridges the gap between abstract, distributed data and related algorithms such as MapReduce and the SQL we all learned
to love and hate.</p><h2 id=hbase-and-other-non-relational-databases>HBase and other non-relational databases</h2><p>But obviously, there is a world beyond relational data. Enter the world of “noSQL”.</p><p>If you ever had the chance to use a production HBase cluster, there’s pretty much two scenarios. One: You love it
because it was used exactly like it was meant to. Two: You hate everybody committing to it and burn
Lars’ <a href=http://www.hbasebook.com/>book</a>
on occasion (which, by the way, I can only highly recommend).</p><p>Non-relational systems, especially the Column-oriented varieties, like HBase, MongoDB, or Accumulo are a fantastic
concept. Usually, there is no concept of ACID transactions, but are hellishly fast if used right.</p><p>If we use HBase, a de-facto standard on Hadoop, as an example, we can quickly see what “used right” means and apply some
common access patterns on our NYC cab data. For details on HBase’s architecture, I will again refer to Lars George’s
book, “HBase – the Definitive Guide” (O’Reilly Media).</p><p>In general terms, noSQL systems heavily rely on the concept of a key – and therefore, doing exactly what Hive omits for
good reasons. Now, what is true about Hive – that it does not come with a fancy client – is especially true for HBase.</p><p>A seemingly simple query like “get me all records where X” turns into a long, horrible orgy of sub-commands over a
charming black-and-white CLI.</p><p>We’ll re-use our NYC Yellow Cab data set (the Hive import from above moves the data set, so you will need to copy it to
HDFS again).</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=n>hbase</span> <span class=n>shell</span> <span class=o>&lt;&lt;&lt;</span> <span class=s2>&#34;create &#39;nyc&#39;, &#39;cf&#39;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>dos2unix</span> <span class=n>yellow_tripdata_2017</span><span class=o>-</span><span class=mo>06</span><span class=n>_id</span><span class=o>.</span><span class=n>csv</span>
</span></span><span class=line><span class=cl><span class=n>hdfs</span> <span class=n>dfs</span> <span class=o>-</span><span class=n>copyFromLocal</span> <span class=n>yellow_tripdata_2017</span><span class=o>-</span><span class=mo>06</span><span class=n>_id</span><span class=o>.</span><span class=n>csv</span> <span class=sr>/user/</span><span class=n>cloudera</span>
</span></span><span class=line><span class=cl><span class=n>hbase</span> <span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>hadoop</span><span class=o>.</span><span class=n>hbase</span><span class=o>.</span><span class=n>mapreduce</span><span class=o>.</span><span class=n>ImportTsv</span> <span class=o>-</span><span class=no>Dimporttsv</span><span class=o>.</span><span class=n>separator</span><span class=o>=</span><span class=p>,</span> <span class=o>-</span><span class=no>Dimporttsv</span><span class=o>.</span><span class=n>columns</span><span class=o>=</span><span class=s2>&#34;HBASE_ROW_KEY,cf:
</span></span></span><span class=line><span class=cl><span class=s2>VendorId,cf:tpep_pickup_datetime,cf:tpep_dropoff_datetime,cf:passenger_count,cf:trip_distance,cf:RatecodeID,cf:
</span></span></span><span class=line><span class=cl><span class=s2>store_and_fwd_flag,cf:PULocationID,cf:DOLocationID,cf:payment_type,cf:fare_amount,cf:extra,cf:mta_tax,cf:tip_amount,cf:
</span></span></span><span class=line><span class=cl><span class=s2>tolls_amount,cf:improvement_surcharge,cf:total_amount&#34;</span> <span class=n>nyc</span> <span class=sr>/user/</span><span class=n>cloudera</span><span class=o>/</span><span class=n>yellow_tripdata_2017</span><span class=o>-</span><span class=mo>06</span><span class=n>_id</span><span class=o>.</span><span class=n>csv</span></span></span></code></pre></td></tr></table></div></div><p>One might add that the initial import took plenty of time on the Quickstart Configuration, which, granted, is not very
good. The command also exits with Bytes Written=0 if something is wrong (like a mismatch in the columns) – do not pass
Go. Do not collect$200.</p><p>Again, let’s get all trips with a tip > $100.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=nb>scan</span> <span class=s1>&#39;nyc&#39;</span><span class=p>,</span> <span class=p>{</span><span class=no>LIMIT</span> <span class=o>=&gt;</span> <span class=mi>10</span><span class=p>,</span> <span class=no>FILTER</span> <span class=o>=&gt;</span> <span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>hadoop</span><span class=o>.</span><span class=n>hbase</span><span class=o>.</span><span class=n>filter</span><span class=o>.</span><span class=n>SingleColumnValueFilter</span><span class=o>.</span><span class=n>new</span><span class=p>(</span><span class=no>Bytes</span><span class=o>.</span><span class=n>toBytes</span><span class=p>(</span><span class=s1>&#39;cf&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=no>Bytes</span><span class=o>.</span><span class=n>toBytes</span><span class=p>(</span><span class=s1>&#39;tip_amount&#39;</span><span class=p>),</span> <span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>hadoop</span><span class=o>.</span><span class=n>hbase</span><span class=o>.</span><span class=n>filter</span><span class=o>.</span><span class=n>CompareFilter</span><span class=o>::</span><span class=no>CompareOp</span><span class=o>.</span><span class=n>valueOf</span><span class=p>(</span><span class=s1>&#39;GREATER_OR_EQUAL&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=no>Bytes</span><span class=o>.</span><span class=n>toBytes</span><span class=p>(</span><span class=mi>100</span><span class=p>))}</span></span></span></code></pre></td></tr></table></div></div><p>What’s that?</p><p><img src=images/Selection_049-1024x644.png alt>
HBase Query Result</p><p>What happened here? Using the TSV import command, all the values are imported as Strings – and we are trying to compare
Strings with Bytes. Technically correct, but not very insightful. There is a whole lot more to the way HBase stores its
data – I will not go into that at this point. But just keep in mind that HBase does not really care about the data you
throw at it.</p><p>So, let’s use Java, the next low-level abstraction HBase offers. We implement a custom Comparator to work with Double in
String fields, move that jar to HBase’s classpath, restart HBase and run our query as jar file. Make sure to set
<code>hbase.client.scanner.timeout.period hbase.regionserver.lease.period</code> accordingly.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kn>package</span><span class=w> </span><span class=nn>com.otterinasuit.hbase</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kn>import</span><span class=w> </span><span class=nn>com.otterinasuit.hbase.comparators.ByteToDoubleComparator</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kn>import</span><span class=w> </span><span class=nn>org.apache.hadoop.hbase.client.*</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kn>import</span><span class=w> </span><span class=nn>org.apache.hadoop.hbase.filter.CompareFilter</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kn>import</span><span class=w> </span><span class=nn>org.apache.hadoop.hbase.filter.LongComparator</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kn>import</span><span class=w> </span><span class=nn>org.apache.hadoop.hbase.filter.SingleColumnValueFilter</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// import ...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>public</span><span class=w> </span><span class=kd>class</span> <span class=nc>Main</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>private</span><span class=w> </span><span class=kd>final</span><span class=w> </span><span class=kd>static</span><span class=w> </span><span class=n>Logger</span><span class=w> </span><span class=n>logger</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>LoggerFactory</span><span class=p>.</span><span class=na>getLogger</span><span class=p>(</span><span class=n>Main</span><span class=p>.</span><span class=na>class</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>private</span><span class=w> </span><span class=kd>final</span><span class=w> </span><span class=kd>static</span><span class=w> </span><span class=kt>double</span><span class=w> </span><span class=n>THRESHOLD</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>100</span><span class=p>.</span><span class=na>00D</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kd>static</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span><span class=w> </span><span class=n>args</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>try</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=kt>long</span><span class=w> </span><span class=n>lStartTime</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>System</span><span class=p>.</span><span class=na>nanoTime</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>logger</span><span class=p>.</span><span class=na>info</span><span class=p>(</span><span class=s>&#34;Start&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>Configuration</span><span class=w> </span><span class=n>config</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Configuration</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>config</span><span class=p>.</span><span class=na>addResource</span><span class=p>(</span><span class=s>&#34;/etc/hbase/conf.cloudera.hbase/hbase-site.xml&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>config</span><span class=p>.</span><span class=na>set</span><span class=p>(</span><span class=s>&#34;hbase.rpc.timeout&#34;</span><span class=p>,</span><span class=w> </span><span class=s>&#34;1800000&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>config</span><span class=p>.</span><span class=na>set</span><span class=p>(</span><span class=s>&#34;hbase.regionserver.lease.period&#34;</span><span class=p>,</span><span class=w> </span><span class=s>&#34;1800000&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>Connection</span><span class=w> </span><span class=n>connection</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>ConnectionFactory</span><span class=p>.</span><span class=na>createConnection</span><span class=p>(</span><span class=n>config</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>Table</span><span class=w> </span><span class=n>table</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>connection</span><span class=p>.</span><span class=na>getTable</span><span class=p>(</span><span class=n>TableName</span><span class=p>.</span><span class=na>valueOf</span><span class=p>(</span><span class=s>&#34;nyc&#34;</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// Filter</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>SingleColumnValueFilter</span><span class=w> </span><span class=n>filter</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>SingleColumnValueFilter</span><span class=p>(</span><span class=n>Bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=p>.</span><span class=na>toBytes</span><span class=p>(</span><span class=s>&#34;cf&#34;</span><span class=p>),</span><span class=w> </span><span class=n>Bytes</span><span class=p>.</span><span class=na>toBytes</span><span class=p>(</span><span class=s>&#34;tip_amount&#34;</span><span class=p>),</span><span class=w> </span><span class=n>CompareFilter</span><span class=p>.</span><span class=na>CompareOp</span><span class=p>.</span><span class=na>GREATER_OR_EQUAL</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=k>new</span><span class=w> </span><span class=n>ByteToDoubleComparator</span><span class=p>(</span><span class=n>THRESHOLD</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// Scan</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>Scan</span><span class=w> </span><span class=n>scan</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Scan</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>scan</span><span class=p>.</span><span class=na>setFilter</span><span class=p>(</span><span class=n>filter</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>scan</span><span class=p>.</span><span class=na>addFamily</span><span class=p>(</span><span class=n>Bytes</span><span class=p>.</span><span class=na>toBytes</span><span class=p>(</span><span class=s>&#34;cf&#34;</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// Run</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=kt>long</span><span class=w> </span><span class=n>c</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>0</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>ResultScanner</span><span class=w> </span><span class=n>scanner</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>table</span><span class=p>.</span><span class=na>getScanner</span><span class=p>(</span><span class=n>scan</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>TreeMap</span><span class=o>&lt;</span><span class=n>Double</span><span class=p>,</span><span class=n>result</span><span class=o>&gt;</span><span class=w> </span><span class=n>list</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>TreeMap</span><span class=o>&lt;</span><span class=n>Double</span><span class=p>,</span><span class=n>result</span><span class=o>&gt;</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>for</span><span class=w> </span><span class=p>(</span><span class=n>Result</span><span class=w> </span><span class=n>r</span><span class=w> </span><span class=p>:</span><span class=w> </span><span class=n>scanner</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>String</span><span class=w> </span><span class=n>total</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>Bytes</span><span class=p>.</span><span class=na>toString</span><span class=p>(</span><span class=n>r</span><span class=p>.</span><span class=na>getValue</span><span class=p>(</span><span class=n>Bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                        </span><span class=p>.</span><span class=na>toBytes</span><span class=p>(</span><span class=s>&#34;cf&#34;</span><span class=p>),</span><span class=w> </span><span class=n>Bytes</span><span class=p>.</span><span class=na>toBytes</span><span class=p>(</span><span class=s>&#34;tip_amount&#34;</span><span class=p>)));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>String</span><span class=w> </span><span class=n>tip</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>Bytes</span><span class=p>.</span><span class=na>toString</span><span class=p>(</span><span class=n>r</span><span class=p>.</span><span class=na>getValue</span><span class=p>(</span><span class=n>Bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                        </span><span class=p>.</span><span class=na>toBytes</span><span class=p>(</span><span class=s>&#34;cf&#34;</span><span class=p>),</span><span class=w> </span><span class=n>Bytes</span><span class=p>.</span><span class=na>toBytes</span><span class=p>(</span><span class=s>&#34;total_amount&#34;</span><span class=p>)));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;tip_amount: &#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>tip</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;total_amount: &#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>total</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=kt>double</span><span class=w> </span><span class=n>total_amount</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>Double</span><span class=p>.</span><span class=na>parseDouble</span><span class=p>(</span><span class=n>total</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=kt>double</span><span class=w> </span><span class=n>tip_amount</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>Double</span><span class=p>.</span><span class=na>parseDouble</span><span class=p>(</span><span class=n>tip</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>list</span><span class=p>.</span><span class=na>put</span><span class=p>(</span><span class=n>tip_amount</span><span class=p>,</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>result</span><span class=p>(</span><span class=n>total_amount</span><span class=p>,</span><span class=n>tip_amount</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>c</span><span class=o>++</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// Calculate in the client</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=kt>double</span><span class=w> </span><span class=n>max_tip</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>Collections</span><span class=p>.</span><span class=na>max</span><span class=p>(</span><span class=n>list</span><span class=p>.</span><span class=na>keySet</span><span class=p>());</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>result</span><span class=w> </span><span class=n>max_res</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>list</span><span class=p>.</span><span class=na>get</span><span class=p>(</span><span class=n>max_tip</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;Max tip was &#34;</span><span class=o>+</span><span class=n>max_res</span><span class=p>.</span><span class=na>getTotal</span><span class=p>()</span><span class=o>+</span><span class=s>&#34;, amount was &#34;</span><span class=o>+</span><span class=n>max_res</span><span class=p>.</span><span class=na>tip</span><span class=o>+</span><span class=s>&#34;, ratio: &#34;</span><span class=o>+</span><span class=n>max_res</span><span class=p>.</span><span class=na>getRatio</span><span class=p>());</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;A total of &#34;</span><span class=o>+</span><span class=n>c</span><span class=o>+</span><span class=s>&#34; people tipped over &#34;</span><span class=o>+</span><span class=n>THRESHOLD</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// Cleanup</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>table</span><span class=p>.</span><span class=na>close</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>connection</span><span class=p>.</span><span class=na>close</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=kt>long</span><span class=w> </span><span class=n>lEndTime</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>System</span><span class=p>.</span><span class=na>nanoTime</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=kt>long</span><span class=w> </span><span class=n>total</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>(</span><span class=n>lEndTime</span><span class=w> </span><span class=o>-</span><span class=w> </span><span class=n>lStartTime</span><span class=p>)</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=n>1000000</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;Rumtime: &#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>total</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=s>&#34;ms / &#34;</span><span class=o>+</span><span class=n>total</span><span class=o>/</span><span class=n>1000</span><span class=o>/</span><span class=n>60</span><span class=o>+</span><span class=s>&#34;min&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w> </span><span class=k>catch</span><span class=w> </span><span class=p>(</span><span class=n>Exception</span><span class=w> </span><span class=n>e</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>logger</span><span class=p>.</span><span class=na>error</span><span class=p>(</span><span class=s>&#34;Reading from HBase failed!&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>e</span><span class=p>.</span><span class=na>printStackTrace</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>static</span><span class=w> </span><span class=kd>class</span> <span class=nc>result</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// ...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>Implementing the custom Comparator sounds like a simple exercise, but believe me, I had to read a lot of HBase source
code in order to get that working. Also, Protobuf. We’ll forge that into a separate article.</p><p><img src=images/word-image-3.png alt></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Max tip was 440.0, amount was 473.3, ratio: 0.9296429326008874
</span></span><span class=line><span class=cl>A total of <span class=m>185</span> people tipped over 100.0
</span></span><span class=line><span class=cl>Rumtime: 284765ms / 4.7min</span></span></code></pre></td></tr></table></div></div><p>A total of 185 people tipped over 100.0</p><p><strong>About 5x the runtime of Hive</strong>. I need to add at this point that using a HBase client application is not a very
efficient way of working with this data in itself and that I threw together that jar in a couple of minutes (which it
why it screams &ldquo;Kill me!&rdquo;) – but since my test environment is running on a single node anyways, we will let this slide.</p><p>However, this comes back to my point about keys and hence data access patterns. We are using a monotonically increasing,
artificial, meaningless key. It’s like assigning each record in a pile of loose vinyls a increasing number and expecting
to find something quick. Sure, it works eventually – but not efficiently.</p><p>Let me demonstrate this by getting the (chosen-at-random) key “200000”:</p><p><img src=images/word-image-4.png alt>
HBase get\</p><p><strong>0.24s isn’t bad at all</strong>. Running a similar query in our (seemingly) quicker Hive table from above looks like this:</p><p><img src=images/word-image-6.png alt>
Hive HBase &ldquo;get&rdquo;</p><p><strong>34.4s</strong>! In other words, 141x HBase’s runtime.</p><p>The same finding applies for scanning multiple rows – using</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=nb>scan</span> <span class=s1>&#39;nyc&#39;</span><span class=p>,</span> <span class=p>{</span><span class=no>STARTROW</span> <span class=o>=&gt;</span> <span class=s1>&#39;200000&#39;</span><span class=p>,</span> <span class=no>ENDROW</span> <span class=o>=&gt;</span> <span class=s1>&#39;200020&#39;</span><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>to get all rows from 200,000 to 200,020 takes 1.9s in my example, the majority of what would be printing to the shell.</p><p>In real life, we would probably use a meaningful HBase key, such as the trip’s time if we are interested in analyzing
the data based on date and time (on nanosecond precision in order to avoid duplicates). We could ask HBase “get me all
trips from 2017-07-04 18:00:00.000 to 2017-07-05 01:59:59.000 and show me the average fare” if we want to take a look at
4th July fares at night.</p><p>Anyways – while HBase also answers my initial query, the Java method is highly inefficient as well:</p><ul><li>The query runs client-side (as mentioned above)</li><li>We needed to essentially customize HBase or at least work around our data type issues (there’s other ways to achieve
this, but your choice of technology will eventually limit your ability to gain these insights)</li><li>We need to write a custom program in Java</li><li>While it is almost easier than daisy-chaining commands in a shell, it is still fairly complex for such a simple query</li></ul><h2 id=sql-on-nosql>SQL on noSQL</h2><p>But what about SQL? Well, you can use Hive again by pointing an external table to HBase – I just don’t recommend it for
this use case. First, we need an external table (a table that does not manage data, but rather just points to an
existing data source), mapping our HBase columns to Hive columns:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>EXTERNAL</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>hbaseNyc</span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>VendorID</span><span class=w> </span><span class=nb>BIGINT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tpep_pickup_datetime</span><span class=w> </span><span class=k>TIMESTAMP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tpep_dropoff_datetime</span><span class=w> </span><span class=k>TIMESTAMP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>passenger_count</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>trip_distance</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>RatecodeID</span><span class=w> </span><span class=n>STRING</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>store_and_fwd_flag</span><span class=w> </span><span class=n>STRING</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>PULocationID</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>DOLocationID</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>payment_type</span><span class=w> </span><span class=nb>INT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>fare_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>extra</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>mta_tax</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tip_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>tolls_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>improvement_surcharge</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>,</span><span class=n>total_amount</span><span class=w> </span><span class=n>DOUBLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>STORED</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=s1>&#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WITH</span><span class=w> </span><span class=n>SERDEPROPERTIES</span><span class=w> </span><span class=p>(</span><span class=s2>&#34;hbase.columns.mapping&#34;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s2>&#34;
</span></span></span><span class=line><span class=cl><span class=s2>:key
</span></span></span><span class=line><span class=cl><span class=s2>,cf:tpep_pickup_datetime
</span></span></span><span class=line><span class=cl><span class=s2>,cf:tpep_dropoff_datetime
</span></span></span><span class=line><span class=cl><span class=s2>,cf:passenger_count
</span></span></span><span class=line><span class=cl><span class=s2>,cf:trip_distance
</span></span></span><span class=line><span class=cl><span class=s2>,cf:RatecodeID
</span></span></span><span class=line><span class=cl><span class=s2>,cf:store_and_fwd_flag
</span></span></span><span class=line><span class=cl><span class=s2>,cf:PULocationID
</span></span></span><span class=line><span class=cl><span class=s2>,cf:DOLocationID
</span></span></span><span class=line><span class=cl><span class=s2>,cf:payment_type
</span></span></span><span class=line><span class=cl><span class=s2>,cf:fare_amount
</span></span></span><span class=line><span class=cl><span class=s2>,cf:extra
</span></span></span><span class=line><span class=cl><span class=s2>,cf:mta_tax
</span></span></span><span class=line><span class=cl><span class=s2>,cf:tip_amount
</span></span></span><span class=line><span class=cl><span class=s2>,cf:tolls_amount
</span></span></span><span class=line><span class=cl><span class=s2>,cf:improvement_surcharge
</span></span></span><span class=line><span class=cl><span class=s2>,cf:total_amount
</span></span></span><span class=line><span class=cl><span class=s2>&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>TBLPROPERTIES</span><span class=w> </span><span class=p>(</span><span class=s1>&#39;hbase.table.name&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;nyc&#39;</span><span class=p>);</span></span></span></code></pre></td></tr></table></div></div><p>The query certainly is easier to manage, but the performance is also not great either:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=m>1</span> 440.0 473.3
</span></span><span class=line><span class=cl>Time taken: 265.434 seconds, Fetched: <span class=m>155</span> row<span class=o>(</span>s<span class=o>)</span>
</span></span><span class=line><span class=cl>4.4 minutes.</span></span></code></pre></td></tr></table></div></div><p>And the explanation remains the same: Column-based random-access is not what HBase was built for.</p><p>HBase is really fast on any access that involves a key, on a lot of inserts and even “updates” (which are really
overwrites), can manage billions of rows, and is incredible flexible, as it does not dictate a fixed schema apart from
ColumnFamilies. You can literally make up columns and data types as you go.</p><p>One alternative is Apache Phoenix, an SQL layer on top of HBase that takes care of a lot of optimization for you. I will
not go into detail at this point – but the key finding (pun intended) remains the same. Using HBase while expecting
random-read queries is a bad idea!</p><p>I guess what I am trying to say – usability is not HBase’s primary target. And it doesn’t have to be. As my “ID 200,000”
example shows, HBase wipes the floor with Hive when it comes to very precise lookups, it’s flexibility due to the lack
of schema is incredible and it will not let you down on very large data sets – if you make sure you know exactly what
you are after.</p><h2 id=apache-spark>Apache Spark</h2><p>First off, Apache Spark is a general-purpose processing engine. Comparing it to other technologies is not really a good
start, as it serves many purposes, such as -</p><ul><li>Programming interface for data access, ETL, Machine Learning and more</li><li>Execution Engine (for Hive)</li><li>Streaming data</li><li>Machine Learning</li></ul><p>I’ve used Spark many times and it remains one of my favorite, to-go tools in the Big Data zoo – mostly because it can do
almost anything and is usually very fast, due to some very smart optimizations and techniques under the hood.</p><p>Without further ado, we will replace M/R2 from the Hive example above with the Scala-based Spark shell (basically, a
live shell you can get by running $ spark-shell) and fire the same query:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>def</span> <span class=n>show_timing</span><span class=o>[</span><span class=kt>T</span><span class=o>](</span><span class=n>proc</span><span class=k>:</span> <span class=o>=&gt;</span> <span class=n>T</span><span class=o>)</span><span class=k>:</span> <span class=kt>T</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>start</span><span class=k>=</span><span class=nc>System</span><span class=o>.</span><span class=n>nanoTime</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>res</span> <span class=k>=</span> <span class=n>proc</span> <span class=c1>// call the code
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>end</span> <span class=k>=</span> <span class=nc>System</span><span class=o>.</span><span class=n>nanoTime</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=n>println</span><span class=o>(</span><span class=s>&#34;Time elapsed: &#34;</span> <span class=o>+</span> <span class=o>(</span><span class=n>end</span><span class=o>-</span><span class=n>start</span><span class=o>)/</span><span class=mi>1000</span><span class=o>/</span><span class=mi>1000</span> <span class=o>+</span> <span class=s>&#34; ms&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>res</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>sql</span> <span class=k>=</span> <span class=s>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>with tmp as(
</span></span></span><span class=line><span class=cl><span class=s>SELECT * FROM Nyc
</span></span></span><span class=line><span class=cl><span class=s>WHERE tip_amount &gt;= 100
</span></span></span><span class=line><span class=cl><span class=s>)
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>SELECT COUNT(*), max(tip_amount), total_amount as max_tip from tmp group by total_amount&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>show_timing</span><span class=o>(</span><span class=n>sqlContext</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span><span class=n>sql</span><span class=o>).</span><span class=n>show</span><span class=o>(</span><span class=mi>200</span><span class=o>))</span></span></span></code></pre></td></tr></table></div></div><p><img src=images/word-image-8.png alt>
Spark</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Time elapsed: <span class=m>12420</span> ms</span></span></code></pre></td></tr></table></div></div><p>12s for the exact same result. Of course – we already spent time on Hive prior to this, but the same result with a
slightly different performance outcome could be achieved by reading the CSV from HDFS directly! Using Spark 2 makes this
easy, but we can use the default 1.6 as well, as installing Spark 2 on Cloudera be a bit tricky.</p><p>So we use the <a href=https://github.com/databricks/spark-csv target=_blank rel="nofollow noopener noreferrer">databricks-csv</a>
library:
<code>spark-shell --packages com.databricks:spark-csv\_2.10:1.5.0</code></p><p>And this bit of code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>import</span> <span class=nn>org.apache.spark.sql.SQLContext</span>
</span></span><span class=line><span class=cl><span class=k>import</span> <span class=nn>org.apache.spark.sql.types._</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>show_timing</span><span class=o>[</span><span class=kt>T</span><span class=o>](</span><span class=n>proc</span><span class=k>:</span> <span class=o>=&gt;</span> <span class=n>T</span><span class=o>)</span><span class=k>:</span> <span class=kt>T</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>start</span><span class=k>=</span><span class=nc>System</span><span class=o>.</span><span class=n>nanoTime</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>res</span> <span class=k>=</span> <span class=n>proc</span> <span class=c1>// call the code
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>end</span> <span class=k>=</span> <span class=nc>System</span><span class=o>.</span><span class=n>nanoTime</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=n>println</span><span class=o>(</span><span class=s>&#34;Time elapsed: &#34;</span> <span class=o>+</span> <span class=o>(</span><span class=n>end</span><span class=o>-</span><span class=n>start</span><span class=o>)/</span><span class=mi>1000</span><span class=o>/</span><span class=mi>1000</span> <span class=o>+</span> <span class=s>&#34; ms&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>res</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>schema</span> <span class=k>=</span> <span class=nc>StructType</span><span class=o>(</span><span class=nc>Array</span><span class=o>(</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;id&#34;</span><span class=o>,</span> <span class=nc>IntegerType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;VendorID&#34;</span><span class=o>,</span> <span class=nc>IntegerType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;tpep_pickup_datetime&#34;</span><span class=o>,</span> <span class=nc>StringType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;tpep_dropoff_datetime&#34;</span><span class=o>,</span> <span class=nc>StringType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;passenger_count&#34;</span><span class=o>,</span> <span class=nc>IntegerType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;trip_distance&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;RatecodeID&#34;</span><span class=o>,</span> <span class=nc>StringType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;store_and_fwd_flag&#34;</span><span class=o>,</span> <span class=nc>StringType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;PULocationID&#34;</span><span class=o>,</span> <span class=nc>IntegerType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;DOLocationID&#34;</span><span class=o>,</span> <span class=nc>IntegerType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;payment_type&#34;</span><span class=o>,</span> <span class=nc>IntegerType</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;fare_amount&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;extra&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;mta_tax&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;tip_amount&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;tolls_amount&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;improvement_surcharge&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl><span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;total_amount&#34;</span><span class=o>,</span> <span class=nc>DecimalType</span><span class=o>.</span><span class=nc>Unlimited</span><span class=o>,</span> <span class=kc>true</span><span class=o>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>df</span> <span class=k>=</span> <span class=n>sqlContext</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=o>(</span><span class=s>&#34;com.databricks.spark.csv&#34;</span><span class=o>).</span><span class=n>option</span><span class=o>(</span><span class=s>&#34;header&#34;</span><span class=o>,</span> <span class=s>&#34;true&#34;</span><span class=o>).</span><span class=n>option</span><span class=o>(</span><span class=s>&#34;inferSchema&#34;</span><span class=o>,</span> <span class=s>&#34;false&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>.</span><span class=n>schema</span><span class=o>(</span><span class=n>schema</span><span class=o>).</span><span class=n>load</span><span class=o>(</span><span class=s>&#34;hdfs:///user/cloudera/yellow_tripdata_2017-06_id.csv&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>registerTempTable</span><span class=o>(</span><span class=s>&#34;nycspark&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>sql</span> <span class=k>=</span> <span class=s>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>with tmp as(
</span></span></span><span class=line><span class=cl><span class=s>SELECT * FROM nycspark
</span></span></span><span class=line><span class=cl><span class=s>WHERE tip_amount &gt;= 100
</span></span></span><span class=line><span class=cl><span class=s>)
</span></span></span><span class=line><span class=cl><span class=s>SELECT COUNT(*), max(tip_amount), total_amount as max_tip from tmp group by total_amount&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>show_timing</span><span class=o>(</span><span class=n>sqlContext</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span><span class=n>sql</span><span class=o>).</span><span class=n>show</span><span class=o>(</span><span class=mi>200</span><span class=o>))</span></span></span></code></pre></td></tr></table></div></div><p><img src=images/word-image-11.png alt>
Spark</p><p>As you can see, the performance is not exactly a miracle and we had to manually define a schema, as we removed the CSV
header – but keep in mind that this approach directly reads your CSV from HDFS, basically skipping Hive altogether.
While aggregates might not be the key use case here, this is a very neat feature to work with various files directly on
Spark.</p><p>In order to answer our “200000 key question”, the same query in Hive is not very convincing either:</p><p><img src=images/word-image-12.png alt>
Spark Hive &ldquo;get&rdquo;</p><p>Another 50s.</p><p>And for good measure, the same on the CSV:</p><p><img src=images/word-image-13.png alt>
Spark CSV &ldquo;get&rdquo;</p><p>And even better, knowing just a bit of Scala, we can do much more with that data – we could store it in a DataFrame, do
more analysis on there, write it to disk, connect it to other sources, read from a completely different file from disk
without any involvement of Hive whatsoever, combine those results, transform the code into a Real-Time application and
much more.</p><p>We can use Spark on Python, Scala, Java and R, can run it from a web front-end such as Apache Zeppelin or just build
mighty, full-size applications that involve build servers and more unit tests than you can imagine even in your wildest
dreams.</p><p>But again – while the above example surely is simple, really complex Spark applications will result in full-sized
software projects. But whether it is a simple Script or a complex application, Spark will serve your end users familiar
with SQL (especially on a graphical tool like Zeppelin) as well as your Data Scientists and Engineers.</p><p>Talking about Spark in a couple of paragraphs does not do it justice, but the gist of it is: Simple to use (although not
as intuitive as a Microsoft SQL client) quick results, hard to master, and very powerful – but not exactly Microsoft SQL
Management Studio either.</p><h2 id=conclusion>Conclusion</h2><p>Let’s summarize: Using a fancy toolset doesn’t help anybody if it’s not used accordingly. Not surprising!</p><p>It gets even more apparent when we sum up our benchmarks:</p><p><img src=images/2017-11-03_Performance.png alt>
Performance Overview</p><p>Hadoop does not magically solve your problems. Period. It does, however, provide a very powerful foundation for many
years to come, especially when you think beyond your usual DWH Reporting targets.</p><p>Generally speaking -</p><p>Use <strong>Hive</strong> for “standard” queries – it gets as close to an RDBMS as you’re going to get. With ACID enabled and a smart
use of Bucketing and Partitioning instead of traditional Primary Keys where applicable, using a good choice of ETL
rules (avoiding VARCHAR join conditions, for instance), it serves as powerful, easy-to-use access layer to your Data
Lake, especially in combination with a tool like Dbeaver. Do not expect it to make business users happy without proper
Change Managstrongent.</p><p>It also opens the gate for much more advanced use cases: As starting or end point for your Machine Learning algorithms (
which might very well combine structured and unstructured data), for instance.</p><p><strong>HBase</strong> is clearly targeted at developers, but can also be exposed to end-users – if you know what you are doing.
While amazing at certain things, HBase (and Phoenix) need to be used for use cases where you know your access and
ingestion patterns – but if done right, it will not let you down.</p><p><strong>Spark</strong> got (hopefully) a bit demystified. Spark, ultimately, is an Execution Engine. As such, it is, again, targeted
at developers. However, using smart tools like Zeppelin and the SparkSQL free-form SQL interface, it can surely be used
by many Database Architects and Power Users that are willing to handle a bit more coding than usual.</p><p>Independent of the toolset, the well-known Hadoop benefits still hold true today – runs on commodity hardware,
super-scalable, very robust and Open Source.</p><p>So, is the concept of a “Data Lake” and especially Hadoop right for you? I’ll tell you what I tell all my clients: <strong>It
depends</strong>. But it is certainly worth a look, especially if you expect your data to grow and your reporting needs to
evolve.</p><p>There is a ton of topics I did not even mention in this 4,000 word article – but rest assured that a business that knows
how to make use of more than their standard “reporting” interfaces and ERP- and CRM-extracts will gain a competitive
advantage in the marketplace – as we all know, knowledge is key!</p><p><em>All test were done on a single-node Cloudera Quickstart VM, running on QEMU/KVM with 12 AMD Ryzen 1700 vCores @ 3.2Ghz
and 25GiB RAM, image launched from a 3TB WD RED Raid-1 Array under Fedora 26. yarn.nodemanager.resource.memory-mb was
set to 4GiB, yarn.nodemanager.resource.cpu-vcores to 10</em></p></div><nav class="hide-on-mobile section-nav"><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#utilizing-your-data-lake>Utilizing your Data Lake</a></li><li><a href=#a-short-prefix-for-apache-hive>A short prefix for Apache Hive</a></li><li><a href=#hive>Hive</a></li><li><a href=#hive-on-9656995-records>Hive on 9,656,995 records</a></li><li><a href=#hbase-and-other-non-relational-databases>HBase and other non-relational databases</a></li><li><a href=#sql-on-nosql>SQL on noSQL</a></li><li><a href=#apache-spark>Apache Spark</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></nav></div><div class=post><hr class=footer-separator><div class=tags><ul class=flat><li class=tag-li><a href=../../../tags/big-data>big data</a></li><li class=tag-li><a href=../../../tags/hbase>hbase</a></li><li class=tag-li><a href=../../../tags/hive>hive</a></li><li class=tag-li><a href=../../../tags/spark>spark</a></li><li class=tag-li><a href=../../../tags/scala>scala</a></li><li class=tag-li><a href=../../../tags/phoenix>phoenix</a></li><li class=tag-li><a href=../../../tags/data-lake>data lake</a></li><li class=tag-li><a href=../../../tags/hadoop>hadoop</a></li></ul></div><div class=back><a href=https://github.com/vim89/vitthalmirji.com/blob/master/content/posts/2017/11/data-lakes-some-thoughts-on-hadoop-hive-hbase-and-spark/index.md title=github><i data-feather=github></i> Edit this on GitHub</a></div><div class=back><a href=https://vitthalmirji.com/><span aria-hidden=true>← Back</span></a></div><div class=back>Next time, we'll talk about <i>"What Tiger King can teach us about x86 Assembly"</i></div></div></div><div class="footer wrapper"><nav class=nav><div>2017 © Copyright notice</div></nav></div><script>feather.replace()</script><script>var enableTruncate=!0,filterDepth=!1;const MAX_DEPTH=9;window.addEventListener("DOMContentLoaded",()=>{const e=new IntersectionObserver(e=>{e.reverse().forEach(e=>{const n=e.target.getAttribute("id");if(e.intersectionRatio>0){var t=document.querySelectorAll(`nav li a[href="#${n}"]`);t!=null&&t.forEach(e=>{if(e!=null){var t=getDepth(e.parentElement);filterDepth&&t<=MAX_DEPTH&&(clearActiveStatesInTableOfContents(),e.parentElement.classList.add("active"))}else filterDepth||(clearActiveStatesInTableOfContents(),e.parentElement.classList.add("active"))})}})});document.querySelectorAll("h1[id],h2[id],h3[id],h4[id]").forEach(t=>{e.observe(t)})});function isVisible(e){if(!(e instanceof Element))return!1;const n=getComputedStyle(e);if(n.display==="none")return!1;if(n.visibility!=="visible")return!1;if(n.opacity<.1)return!1;if(e.offsetWidth+e.offsetHeight+e.getBoundingClientRect().height+e.getBoundingClientRect().width===0)return!1;const t={x:e.getBoundingClientRect().left+e.offsetWidth/2,y:e.getBoundingClientRect().top+e.offsetHeight/2};if(t.x<0)return!1;if(t.x>(document.documentElement.clientWidth||window.innerWidth))return!1;if(t.y<0)return!1;if(t.y>(document.documentElement.clientHeight||window.innerHeight))return!1;let s=document.elementFromPoint(t.x,t.y);do if(s===e)return!0;while(s=s.parentNode)return!1}function clearActiveStatesInTableOfContents(){document.querySelectorAll("nav li").forEach(e=>{e.classList.remove("active")})}function getDepth(e){for(var t=0;e!==null&&e.tagName.toLowerCase()!=="ul";)t++,e=e.parentElement;return t}function navItems(){var e=document.querySelectorAll("nav nav li a");return Array.from(e).filter(e=>e.href!=null&&e.hash.startsWith("#"))}function lasItemInNavBarVisible(){var e=navItems().slice(-1)[0];return isVisible(e)}document.addEventListener("DOMContentLoaded",function(){if(!enableTruncate)return;var e=navItems();console.log(e),lasItemInNavBarVisible()||(filterDepth=!0,e.forEach(function(e){var t=getDepth(e.parentElement);t>MAX_DEPTH&&e.parentElement.classList.add("depth-nested")}))})</script></body></html>